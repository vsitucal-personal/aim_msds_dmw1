{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "40b90546-0ebf-4702-a408-4a90e6309036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T17:30:13.742368Z",
     "iopub.status.busy": "2023-12-05T17:30:13.741623Z",
     "iopub.status.idle": "2023-12-05T17:30:13.753758Z",
     "shell.execute_reply": "2023-12-05T17:30:13.751810Z",
     "shell.execute_reply.started": "2023-12-05T17:30:13.742304Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from xml.etree import ElementTree as ET\n",
    "import re\n",
    "from io import StringIO\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.display import HTML\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b26fdee0-53a3-46d2-800a-7a3dc191053b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T17:30:14.208149Z",
     "iopub.status.busy": "2023-12-05T17:30:14.207431Z",
     "iopub.status.idle": "2023-12-05T17:30:14.220679Z",
     "shell.execute_reply": "2023-12-05T17:30:14.218885Z",
     "shell.execute_reply.started": "2023-12-05T17:30:14.208088Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script\n",
       "    src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js \">\n",
       "</script>\n",
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.jp-CodeCell > div.jp-Cell-inputWrapper').hide();\n",
       " } else {\n",
       "$('div.jp-CodeCell > div.jp-Cell-inputWrapper').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\"\n",
       "    value=\"Click here to toggle on/off the raw code.\"></form>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''\n",
    "<script\n",
    "    src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js \">\n",
    "</script>\n",
    "<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.jp-CodeCell > div.jp-Cell-inputWrapper').hide();\n",
    " } else {\n",
    "$('div.jp-CodeCell > div.jp-Cell-inputWrapper').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\"\n",
    "    value=\"Click here to toggle on/off the raw code.\"></form>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699821fd-c190-44ef-9fb9-7764c42ab544",
   "metadata": {},
   "source": [
    "# Central Insights: A Latent Semantic Analysis of Bangko Sentral ng Pilipinas Press Releases\n",
    "Version date: 05 December 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc3412d-c1ab-4268-93f0-a594d1570f34",
   "metadata": {},
   "source": [
    "## Executive summary\n",
    "***\n",
    "\n",
    "In this project, we explore the relevant topics based on the press releases by the Bangko Sentral ng Pilipinas (BSP) from 1999 to 2023. The raw data for the press releases is webscraped from the BSP Media and Research web page. The raw data is then cleaned and prepared and is converted into a bag-of-words design matrix using the Term Frequence-Inverse Document Frequency (TF-IDF) model for the purposes of information retrieval and keyword extraction. The TF-IDF bag-of-words then undergoes Singular Value Decomposition/Latent Semantic Analysis for dimensionality reduction. We then determine latent topics over years from the truncated singular vectors. We also determine standout topics across recent years as well as standout tokens per topic per year to demonstrate the narrative potential of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af15994e-1f60-4a24-ba76-8fcd443dc11b",
   "metadata": {},
   "source": [
    "## 1&emsp;Introduction\n",
    "***\n",
    "In the dynamic landscape of economic policymaking, central banks play a pivotal role in shaping and steering a nation's economic trajectory. They stand at the nexus of economic governance and stability, and they possess unparalleled insights into the economic pulses of their nations. Central banks are thus uniquely positioned to offer rich and comprehensive accounts about their respective nations' economic storylines. One only needs to peruse their central bank's' official publications to catch glimpses of developments in their nation's economy.\n",
    "\n",
    "In this project, we unveil snippets of Philippine central banking history by employing Latent Semantic Analysis (LSA) on the press releases of the Bangko Sentral ng Pilipinas (BSP) as a case study. Through the lens of LSA, we aim to uncover the latent themes surrounding BSP communications, shedding light on implicit associations and connections within its press releases. We seek two answer two questions. First, what are the overarching themes in the historical narrative of Philippine central banking? Second, how have the narrative themes changed year after year?\n",
    "\n",
    "The rest of this report proceeds as follows. Section 2 details the methodology employed to conduct the analysis. Section 3 presents the results and provides a brief discussion. Section 4 concludes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec332b84-0111-4ed2-8a2d-06297527bd04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T09:50:45.176520Z",
     "iopub.status.busy": "2023-11-30T09:50:45.175776Z",
     "iopub.status.idle": "2023-11-30T09:50:45.188066Z",
     "shell.execute_reply": "2023-11-30T09:50:45.185557Z",
     "shell.execute_reply.started": "2023-11-30T09:50:45.176458Z"
    },
    "tags": []
   },
   "source": [
    "## 2&emsp;Motivation\n",
    "***\n",
    "The BSP's mandate stats that its main objective is to maintain price stability conducive to a balanced and sustainable growth of the economy and employment.In the past recent years, specifically in response to the pandemic, have shown their essential function in numerous policy measures to ensure economic stability. These include interest rate cuts to encourage borrowing and spending, liquidity support to financial institutions to ensure smooth and functioning financial markets among others. \n",
    "\n",
    "The central bank’s actions and their focus affects the whole economy of the country. Not to mention the regulatory pillars that they implement and maintain to safeguard the economic stability and attract investors both locally and internationally. \n",
    "This report also aims to tell the focus of the BSP from 1999 2023. Further, it also recognizes the central bank’s function and response amidst of crises and technological adaptation throughout these years. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751046c9-45b6-499d-a985-eb7ba34dc5aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T09:50:45.176520Z",
     "iopub.status.busy": "2023-11-30T09:50:45.175776Z",
     "iopub.status.idle": "2023-11-30T09:50:45.188066Z",
     "shell.execute_reply": "2023-11-30T09:50:45.185557Z",
     "shell.execute_reply.started": "2023-11-30T09:50:45.176458Z"
    },
    "tags": []
   },
   "source": [
    "## 3&emsp;Methodology\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e151a90-cd0e-400c-a8e9-ab5a0182b5f1",
   "metadata": {},
   "source": [
    "In order to conduct our analysis, we perform Latent Semantic Analysis via Singular Value Decomposition (SVD) on the corpus of BSP press releases.\n",
    "\n",
    "First, we extract the corpus of press release data fron the BSP's website via webscraping and API extraction. We then perform data cleanup to clean the response content from the api and then parsed the XML contents into a pandas dataframe. Title and the article content are combined into a single entity and will be the input in constructing the bag of words fed into the TF-IDF vectorizer in the later step. We construct the design matrix we use for analysis by converting the press release data into a term frequency-inverse document frequency bag-of-words. Afterward, we perform dimensionality reduction on the design matrix via SVD/LSA to capture the most important features that explain the variability in the data. Finally, we apply domain knowledge to identify the latent themes from the singular values and extract practical insights. The steps are summarized in Figure 1.\n",
    "\n",
    "<center>Figure 1. Summary of methodology</center>\n",
    "\n",
    "<center><img src=\"methodology.jpg\" alt=\"methodology.jpg\"/></center><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59530b0d-9846-40f2-816e-a4612e95a9fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T16:56:43.659253Z",
     "iopub.status.busy": "2023-12-04T16:56:43.658170Z",
     "iopub.status.idle": "2023-12-04T16:56:43.678086Z",
     "shell.execute_reply": "2023-12-04T16:56:43.676101Z",
     "shell.execute_reply.started": "2023-12-04T16:56:43.659151Z"
    },
    "tags": []
   },
   "source": [
    "### 3.1&emsp;Data collection and cleanup\n",
    "\n",
    "The official website of the BSP serves as the primary repository for the central bank's publication materials. Our subjects of interest and our source, the BSP's press releases, are conveniently lodged in the \"Press Releases\" tab of the \"Media and Research\" page (BSP, n.d.). Figure 2 shows a screenshot of the subject page.\n",
    "\n",
    "<center>Figure 2. The BSP Media and Research webpage</center>\n",
    "\n",
    "<center><img src=\"bsp_media2.png\" alt=\"bsp_media.png\" style=\"width:800px; height:600px\"/></center><br>\n",
    "\n",
    "The web page displays only the 5,000 most recent press releases. As of writing, the oldest press release available for viewing via the web page dates back to April 2007. However, a deeper dive through the web page via webscraping reveals that press releases all the way from 1999 are available are obtainable via API retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c371d5-193d-4a99-8aed-d74bdebe70fd",
   "metadata": {},
   "source": [
    "**Webscraping/API helper functions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "db8c6bc8-142f-4779-b8f5-f8e00367466e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T17:30:43.865908Z",
     "iopub.status.busy": "2023-12-05T17:30:43.865165Z",
     "iopub.status.idle": "2023-12-05T17:30:43.889792Z",
     "shell.execute_reply": "2023-12-05T17:30:43.888353Z",
     "shell.execute_reply.started": "2023-12-05T17:30:43.865845Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleancontent(data):\n",
    "    \"\"\"Clean content\"\"\"\n",
    "    first = re.compile(r'<.*?>')\n",
    "    result = first.sub('', data)\n",
    "    second = re.compile(r'&#.*?;')\n",
    "    result = second.sub('', result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def cleanxml(string):\n",
    "    \"\"\"Clean xml\"\"\"\n",
    "    it = ET.iterparse(StringIO(string))\n",
    "    for _, el in it:\n",
    "        _, _, el.tag = el.tag.rpartition('}')\n",
    "    xml = it.root\n",
    "    return xml\n",
    "\n",
    "\n",
    "def load_bsp_data(year):\n",
    "    \"\"\"Load bsp articles for the given year\"\"\"\n",
    "    url = \"https://www.bsp.gov.ph/_api/web/lists/getByTitle('Media%20\" + \\\n",
    "        \"Releases%20and%20Advisories')/items?\"\n",
    "    filter_string = f\"PDate ge '{year}-01-01T00:00:00.000Z' and \" + \\\n",
    "        f\"PDate le '{year}-11-30T00:00:00.000Z' and Tag eq 'Media\" + \\\n",
    "        \" Releases' and OData__ModerationStatus eq 0 and Status eq '2'\"\n",
    "\n",
    "    params = {\n",
    "      \"$select\": \"*\",\n",
    "      \"$filter\": filter_string,\n",
    "      \"$top\": 5000,\n",
    "      \"$orderby\": \"PDate desc\"\n",
    "    }\n",
    "\n",
    "    data = requests.get(url, params=params)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_bsp_data_years(start_year, end_year):\n",
    "    \"\"\"Load bsp articles given start and end years\"\"\"\n",
    "    dict_entries = {}\n",
    "\n",
    "    while start_year >= end_year:\n",
    "        data = load_bsp_data(start_year)\n",
    "        xml = cleanxml(data.text)\n",
    "        entries = xml.findall('entry')\n",
    "        dates = []\n",
    "        titles = []\n",
    "        content = []\n",
    "        for each_entry in entries:\n",
    "            dates.append(each_entry.findall('.//PDate')[0].text)\n",
    "            titles.append(each_entry.findall('.//Title')[0].text)\n",
    "            content.append(cleancontent(each_entry.findall('.//Content')[0].text))\n",
    "\n",
    "        if 'created_date' not in dict_entries.keys():\n",
    "            dict_entries['created_date'] = dates\n",
    "        else:\n",
    "            dict_entries['created_date'] += dates\n",
    "\n",
    "        if 'title' not in dict_entries.keys():\n",
    "            dict_entries['title'] = titles\n",
    "        else:\n",
    "            dict_entries['title'] += titles\n",
    "\n",
    "        if 'content' not in dict_entries.keys():\n",
    "            dict_entries['content'] = content\n",
    "        else:\n",
    "            dict_entries['content'] += content\n",
    "\n",
    "        start_year -= 1\n",
    "    return dict_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14364154-c0fc-41d6-b5d0-e72ac9771433",
   "metadata": {},
   "source": [
    "**Prepare raw dataframe via webscraping/API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3b8384f9-b736-44db-af12-237104d861c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T17:30:50.744540Z",
     "iopub.status.busy": "2023-12-05T17:30:50.743719Z",
     "iopub.status.idle": "2023-12-05T17:31:19.636374Z",
     "shell.execute_reply": "2023-12-05T17:31:19.635628Z",
     "shell.execute_reply.started": "2023-12-05T17:30:50.744404Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe info: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5905 entries, 0 to 5904\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   created_date   5905 non-null   object\n",
      " 1   title_content  5905 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 92.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe describe: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_date</th>\n",
       "      <th>title_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5905</td>\n",
       "      <td>5905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3374</td>\n",
       "      <td>5905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2010-09-12T16:00:00Z</td>\n",
       "      <td>foreign investments registered with the bangko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                created_date  \\\n",
       "count                   5905   \n",
       "unique                  3374   \n",
       "top     2010-09-12T16:00:00Z   \n",
       "freq                      10   \n",
       "\n",
       "                                            title_content  \n",
       "count                                                5905  \n",
       "unique                                               5905  \n",
       "top     foreign investments registered with the bangko...  \n",
       "freq                                                    1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webscraped dataframe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_date</th>\n",
       "      <th>title_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-11-29T16:00:00Z</td>\n",
       "      <td>foreign investments registered with the bangko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-29T16:00:00Z</td>\n",
       "      <td>month-ahead inflation forecast for november 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-11-27T16:00:00Z</td>\n",
       "      <td>court convicts owner and operator of globexmc ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-11-26T16:00:00Z</td>\n",
       "      <td>calamba city rolls out paleng-qr ph plus ​cala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-11-22T16:00:00Z</td>\n",
       "      <td>bsp joins pbbm at economic briefing in san fra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5900</th>\n",
       "      <td>1999-03-10T16:00:00Z</td>\n",
       "      <td>bsp reminds house minority block:  constitutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5901</th>\n",
       "      <td>1999-02-18T16:00:00Z</td>\n",
       "      <td>kbs loans up, npls down the commercial banking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5902</th>\n",
       "      <td>1999-02-04T16:00:00Z</td>\n",
       "      <td>rise in january inflation rate driven by suppl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5903</th>\n",
       "      <td>1999-02-02T16:00:00Z</td>\n",
       "      <td>status of commercial banking system as of end-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5904</th>\n",
       "      <td>1999-01-05T16:00:00Z</td>\n",
       "      <td>inflation rate for 1998 lower than target bsp ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5905 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              created_date                                      title_content\n",
       "0     2023-11-29T16:00:00Z  foreign investments registered with the bangko...\n",
       "1     2023-11-29T16:00:00Z  month-ahead inflation forecast for november 20...\n",
       "2     2023-11-27T16:00:00Z  court convicts owner and operator of globexmc ...\n",
       "3     2023-11-26T16:00:00Z  calamba city rolls out paleng-qr ph plus ​cala...\n",
       "4     2023-11-22T16:00:00Z  bsp joins pbbm at economic briefing in san fra...\n",
       "...                    ...                                                ...\n",
       "5900  1999-03-10T16:00:00Z  bsp reminds house minority block:  constitutio...\n",
       "5901  1999-02-18T16:00:00Z  kbs loans up, npls down the commercial banking...\n",
       "5902  1999-02-04T16:00:00Z  rise in january inflation rate driven by suppl...\n",
       "5903  1999-02-02T16:00:00Z  status of commercial banking system as of end-...\n",
       "5904  1999-01-05T16:00:00Z  inflation rate for 1998 lower than target bsp ...\n",
       "\n",
       "[5905 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set years to extract\n",
    "start = 2023\n",
    "end = 1999\n",
    "\n",
    "df = pd.DataFrame.from_dict(load_bsp_data_years(start, end))\n",
    "df = df.sort_values('created_date', ascending=False)\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Combine title and content to become a single word soup\n",
    "df['title_content'] = df['title'] + \" \" + df['content']\n",
    "df['title_content'] = df['title_content'].astype(str)\n",
    "df['title_content'] = df['title_content'].str.lower()\n",
    "\n",
    "# Remove title and content columns\n",
    "df = df.drop(columns=['title', 'content'])\n",
    "print(\"dataframe info: \")\n",
    "display(df.info())\n",
    "print(\"dataframe describe: \")\n",
    "display(df.describe())\n",
    "print(\"Webscraped dataframe:\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef899b3-e98b-4701-9d7b-03e28589c67a",
   "metadata": {},
   "source": [
    "### 3.2&emsp;Convert the raw dataframe into a TF-IDF bag-of-words design matrix\n",
    "\n",
    "We then transform the raw data of BSP press releases into a TF-IDF bag-of-words design matrix. TF-IDF is a numerical statistic used in natural language processing and information retrieval to evaluate the importance of a word in a document relative to a collection of documents (corpus). The TF-IDF value for a term is calculated by multiplying its frequency (TF) in a document by the inverse document frequency (IDF) across the entire corpus. The resulting TF-IDF scores form a vector representation of the document, known as a bag-of-words, where each word is assigned a weight reflecting its significance in the context of the document and the corpus as a whole. This approach helps capture the unique characteristics of documents by emphasizing terms that are both frequent within the document and rare across the entire corpus.\n",
    "\n",
    "We first conduct an initial transformation without setting maximum and mininum document frequency and maximum features parameters to get a preliminary sense of the data, resulting in a bag-of-words matrix with 5,905 rows corresponding to the number of documents in the corpus and 20,945 columns corresponding the the features or identified tokens. In order to preserve only the most important tokens without significantly compromising the variability in the data, we are compelled to calibrate the parameters. We find that satisfactory results are achieved by retaining only the 1,000 most important features even without setting minimum and maximum document frequency parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "62062a48-0fa2-4e88-b720-c432f5352b66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T17:31:19.638395Z",
     "iopub.status.busy": "2023-12-05T17:31:19.638154Z",
     "iopub.status.idle": "2023-12-05T17:31:21.492479Z",
     "shell.execute_reply": "2023-12-05T17:31:21.491671Z",
     "shell.execute_reply.started": "2023-12-05T17:31:19.638373Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF bag of words df:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abroad</th>\n",
       "      <th>access</th>\n",
       "      <th>according</th>\n",
       "      <th>account</th>\n",
       "      <th>accounted</th>\n",
       "      <th>accounting</th>\n",
       "      <th>accounts</th>\n",
       "      <th>acquired</th>\n",
       "      <th>acquisition</th>\n",
       "      <th>across</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>workers</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yen</th>\n",
       "      <th>yielded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032237</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5900</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.08201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5901</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5902</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5903</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145940</td>\n",
       "      <td>0.062635</td>\n",
       "      <td>0.064971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5904</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135819</td>\n",
       "      <td>0.089901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5905 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abroad  access  according  account  accounted  accounting  accounts  \\\n",
       "0        0.0     0.0        0.0      0.0   0.000000         0.0  0.000000   \n",
       "1        0.0     0.0        0.0      0.0   0.000000         0.0  0.000000   \n",
       "2        0.0     0.0        0.0      0.0   0.000000         0.0  0.000000   \n",
       "3        0.0     0.0        0.0      0.0   0.000000         0.0  0.059308   \n",
       "4        0.0     0.0        0.0      0.0   0.000000         0.0  0.000000   \n",
       "...      ...     ...        ...      ...        ...         ...       ...   \n",
       "5900     0.0     0.0        0.0      0.0   0.000000         0.0  0.000000   \n",
       "5901     0.0     0.0        0.0      0.0   0.000000         0.0  0.000000   \n",
       "5902     0.0     0.0        0.0      0.0   0.000000         0.0  0.000000   \n",
       "5903     0.0     0.0        0.0      0.0   0.157703         0.0  0.145940   \n",
       "5904     0.0     0.0        0.0      0.0   0.000000         0.0  0.000000   \n",
       "\n",
       "      acquired  acquisition    across  ...  work  workers   working    world  \\\n",
       "0     0.000000     0.000000  0.000000  ...   0.0      0.0  0.000000  0.00000   \n",
       "1     0.000000     0.000000  0.000000  ...   0.0      0.0  0.000000  0.00000   \n",
       "2     0.000000     0.000000  0.000000  ...   0.0      0.0  0.000000  0.00000   \n",
       "3     0.000000     0.000000  0.000000  ...   0.0      0.0  0.032237  0.00000   \n",
       "4     0.000000     0.000000  0.000000  ...   0.0      0.0  0.000000  0.00000   \n",
       "...        ...          ...       ...  ...   ...      ...       ...      ...   \n",
       "5900  0.000000     0.000000  0.040062  ...   0.0      0.0  0.000000  0.08201   \n",
       "5901  0.093061     0.000000  0.000000  ...   0.0      0.0  0.000000  0.00000   \n",
       "5902  0.000000     0.000000  0.000000  ...   0.0      0.0  0.000000  0.00000   \n",
       "5903  0.062635     0.064971  0.000000  ...   0.0      0.0  0.000000  0.00000   \n",
       "5904  0.000000     0.000000  0.000000  ...   0.0      0.0  0.000000  0.00000   \n",
       "\n",
       "      worth     would      year     years  yen   yielded  \n",
       "0       0.0  0.000000  0.078122  0.000000  0.0  0.042002  \n",
       "1       0.0  0.000000  0.000000  0.000000  0.0  0.000000  \n",
       "2       0.0  0.000000  0.000000  0.000000  0.0  0.000000  \n",
       "3       0.0  0.000000  0.014392  0.000000  0.0  0.000000  \n",
       "4       0.0  0.000000  0.000000  0.000000  0.0  0.000000  \n",
       "...     ...       ...       ...       ...  ...       ...  \n",
       "5900    0.0  0.000000  0.000000  0.039423  0.0  0.000000  \n",
       "5901    0.0  0.000000  0.035079  0.000000  0.0  0.000000  \n",
       "5902    0.0  0.000000  0.000000  0.000000  0.0  0.000000  \n",
       "5903    0.0  0.000000  0.070831  0.000000  0.0  0.000000  \n",
       "5904    0.0  0.135819  0.089901  0.000000  0.0  0.000000  \n",
       "\n",
       "[5905 rows x 1000 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify a list of stop words\n",
    "\n",
    "# If not yet downloaded\n",
    "# nltk.download('stopwords')\n",
    "stop = list(stopwords.words('english'))\n",
    "stop.extend(\n",
    "    'a b c d e f g h i j k l m n o p q r s t u v w x y z www us'.split()\n",
    ")\n",
    "# Display stop words\n",
    "# print(stop)\n",
    "\n",
    "# Transform the initial data into a TF-IDF design matrix\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    token_pattern=r'\\b([a-z]+)\\b',\n",
    "    max_features=1000,\n",
    "    #max_df=0.95,\n",
    "    #min_df=0.05,\n",
    "    stop_words=stop,\n",
    "    smooth_idf=True,\n",
    ")\n",
    "tfidf_matrix = vectorizer.fit_transform(df['title_content'])\n",
    "tfidf_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    tfidf_matrix,\n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    ")\n",
    "df_bow = pd.concat([df, tfidf_df], axis=1)\n",
    "X = df_bow.drop(columns=['created_date', 'title_content'])\n",
    "features = X.columns\n",
    "print(\"TF-IDF bag of words df:\")\n",
    "display(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0eb44-d42c-440c-8dce-ce8da37fbc9e",
   "metadata": {},
   "source": [
    "### 3.3&emsp;Singular Value Decomposition and Latent Semantic Analysis\n",
    "\n",
    "We perform SVD/LSA on the TF-IDF bag-of-words data using the `TruncatedSVD` method from the `sklearn` library (Pedregosa et al., 2011). SVD is a mathematical technique used for dimensionality reduction and feature extraction in data analysis. In the context of natural language processing, SVD is often applied to term-document matrices in techniques like LSA. LSA utilizes SVD to transform the original high-dimensional space of terms and documents into a lower-dimensional space, capturing the latent semantic structure in the data. This process helps uncover underlying patterns and relationships between words and documents, allowing for more efficient information retrieval and text analysis in applications such as document clustering and topic modeling.\n",
    "\n",
    "We set the number of components to 100 as prescribed by the documentation of the `TruncatedSVD` method. We explain the statistical diagnostic outputs resulting from this choice in Section 3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "96820b92-299a-402e-a091-f6797a172b40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T17:31:23.509758Z",
     "iopub.status.busy": "2023-12-05T17:31:23.509026Z",
     "iopub.status.idle": "2023-12-05T17:31:30.451344Z",
     "shell.execute_reply": "2023-12-05T17:31:30.449918Z",
     "shell.execute_reply.started": "2023-12-05T17:31:23.509697Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set number of singular values\n",
    "k = 100\n",
    "\n",
    "# Decompose\n",
    "svd = TruncatedSVD(\n",
    "    n_components=k,\n",
    ")\n",
    "X_reduced = svd.fit_transform(X.to_numpy())\n",
    "Sigma = svd.singular_values_\n",
    "V_T = svd.components_.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116062bd-fe7c-49bc-9e2d-7d93953504df",
   "metadata": {},
   "source": [
    "### 3.4&emsp;Identification of latent themes\n",
    "\n",
    "Our final step is to extract value from the model by identifying the latent themes embedded within the corpus of BSP's press releases. First, we identify the latent concepts prevalent across the entire corpus of BSP's press releases. We then focus on the latent themes per year by applying the methodology on subsets of the data specified by year starting from 2013. We also show how zooming in on how the tokens within a recurring latent concept change represents a shift in the narrative regarding that specific theme. We discuss the results in Section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3f4a32-4aa7-4d41-aa6b-45f8d67f290d",
   "metadata": {},
   "source": [
    "## 4&emsp;Results and discussion\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f18989-567b-4b03-b1fa-5de74a00b577",
   "metadata": {},
   "source": [
    "In this section we present the results of our model. First, we present the latent concepts identified using the corpus of press releases from 1999 to 2023 to capture an overall idea of what the BSP talks about in its press releases. Second, we use year-specific subsets of the corpus to see what themes stand out every year, allowing us to determine changes in BSP's communication priorities over time. Finally, as a demonstration of the value of our model, we zoom in on a specific latent concept recurring over time and examine how the tokens within the latent concept change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf7c2d-d54d-4a5a-b33d-5fde8f4c3611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T09:56:16.536239Z",
     "iopub.status.busy": "2023-11-30T09:56:16.535483Z",
     "iopub.status.idle": "2023-11-30T09:56:16.547474Z",
     "shell.execute_reply": "2023-11-30T09:56:16.544659Z",
     "shell.execute_reply.started": "2023-11-30T09:56:16.536174Z"
    },
    "tags": []
   },
   "source": [
    "### 4.1&emsp;Preliminaries\n",
    "\n",
    "Figure 3 shows the scatter plot of the reduced TF-IDF matrix across the first two singular vectors.\n",
    "\n",
    "<center>Figure 3. Scatter plot of reduced TF-IDF matrix across first two singular vectors</center>\n",
    "\n",
    "<center><img src=\"scatterplotsv.png\" alt=\"scatterplotsv.png\"/></center><br><center>\n",
    "\n",
    "Taking the top 100 singular values yields a cumulative variance explained of 66.57 percent, which we accept as satisfactory being that more than half of the variance of the truncated data (retaining 1,000 features) is explained.\n",
    "\n",
    "<center>Figure 4. Cumulative variance explained at 100 singular values</center>\n",
    "\n",
    "<center><img src=\"cumvarexplained.png\" alt=\"cumvarexplained.png\"/></center><br><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "78125c01-2b02-453c-8924-da1479b70749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T17:18:35.359907Z",
     "iopub.status.busy": "2023-12-05T17:18:35.359117Z",
     "iopub.status.idle": "2023-12-05T17:18:35.368210Z",
     "shell.execute_reply": "2023-12-05T17:18:35.366311Z",
     "shell.execute_reply.started": "2023-12-05T17:18:35.359842Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN CELL TO DEVELOP SCATTERPLOT\n",
    "\n",
    "#plt.scatter(X_reduced[:, 0], X_reduced[:, 1], alpha=0.5)\n",
    "#plt.title('Scatter Plot of Reduced TF-IDF Matrix')\n",
    "#plt.xlabel('SV 1')\n",
    "#plt.ylabel('SV 2')\n",
    "#plt.tight_layout()\n",
    "#plt.savefig('scatterplotsv.png')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "368b0c33-fec1-4971-84e2-fe0772978a69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T16:53:53.841031Z",
     "iopub.status.busy": "2023-12-05T16:53:53.839224Z",
     "iopub.status.idle": "2023-12-05T16:53:53.848800Z",
     "shell.execute_reply": "2023-12-05T16:53:53.846656Z",
     "shell.execute_reply.started": "2023-12-05T16:53:53.840953Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN CELL TO DEVELOP CUMULATIVE VARIANCE EXPLAINED\n",
    "\n",
    "#print(f\"Cumulative variance explained at {k} retained singular values: {(svd.explained_variance_ratio_*100).cumsum()[-1]:.2f}%\")\n",
    "\n",
    "#plt.plot((svd.explained_variance_ratio_*100).cumsum())\n",
    "#plt.title('Smoothed Cumulative Variance Explained by Truncated SVD')\n",
    "#plt.xlabel('Number of Singular Values')\n",
    "#plt.ylabel('Cumulative Variance Explained')\n",
    "#plt.tight_layout()\n",
    "#plt.savefig('cumvarexplained.png')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b4658-d4d3-4a99-ad45-0e37a9ca16d5",
   "metadata": {},
   "source": [
    "### 4.2&emsp;Top 10 latent themes identified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009abf2e-1371-4c35-be10-ecaddf4d4281",
   "metadata": {},
   "source": [
    "As a demonstration of the analytical potential of LSA, we examine the top 10 groups of words and see if we can discern their respective topics from a human standpoint. (The top 10 singular vectors account for 30.67 percent of cumulative variance.) Because the LSA is an unsupervised learning technique, we do not expect all of the 100 latent concepts identified through the mathematical underpinnings of the model to be human-interpretable (Ioana, 2020). We may also find some overlaps in the latent themes identified by the model, especially at lower singular values. Again, we accept these limitations as given knowing that the BSP deals with very specific aspects of economic governance, and there is only so much that it can talk about in its press releases given the clearly defined scope under which it operates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f8f67ce7-e01d-4fb9-9dad-e77027433d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T17:32:04.222845Z",
     "iopub.status.busy": "2023-12-05T17:32:04.222044Z",
     "iopub.status.idle": "2023-12-05T17:32:04.233077Z",
     "shell.execute_reply": "2023-12-05T17:32:04.230916Z",
     "shell.execute_reply.started": "2023-12-05T17:32:04.222770Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN CELL TO DEVELOP SV CHARTS\n",
    "\n",
    "#feature_names = vectorizer.get_feature_names_out()\n",
    "#for i in range(20):\n",
    "#    fig, ax = plt.subplots()\n",
    "#    order = np.argsort(np.abs(svd.components_.T[:, i]))[-20:]\n",
    "#    ax.barh([feature_names[o] for o in order], svd.components_.T[order, i])\n",
    "#    ax.set_title(f'SV{i+1}')\n",
    "#    fig.tight_layout()\n",
    "#    plt.savefig(f'sv{i+1}_final.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "0ed7eb19-56bf-419c-9e76-14062b4ccb57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T03:15:15.634989Z",
     "iopub.status.busy": "2023-12-05T03:15:15.634189Z",
     "iopub.status.idle": "2023-12-05T03:15:15.646795Z",
     "shell.execute_reply": "2023-12-05T03:15:15.644976Z",
     "shell.execute_reply.started": "2023-12-05T03:15:15.634878Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN CELL TO DEVELOP LATENT THEME CLUSTERS, POSITIVE ASSOCIATIONS\n",
    "\n",
    "#for i, comp in enumerate(svd.components_[:20]):\n",
    "#    terms_comp = zip(features, comp)\n",
    "#    sorted_terms = sorted(\n",
    "#        terms_comp, key=lambda x: x[1], reverse=True\n",
    "#    )[:10]\n",
    "#    print(\"Topic \"+str(i+1)+\": \")\n",
    "#    soup = [i[0] for i in sorted_terms]\n",
    "#    print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c62339-19a4-4a68-a939-4dad6a4034ed",
   "metadata": {},
   "source": [
    "The first latent concept (Figure 5) consists of a hodgepodge of frequently used words by the central bank in its core press releases. Besides this, there does not seem to be a specific underlying concept that unifies the words in this cluster. We thus interpret this cluster as the mother topic of all BSP press releases: central banking.\n",
    "\n",
    "<center>Figure 5. Central banking latent concept</center>\n",
    "\n",
    "<center><img src=\"sv1_final.png\" alt=\"sv1_final.png\" style=\"width:800px; height:600px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f2fbd-20aa-4c1f-ab0a-94693b0746d8",
   "metadata": {},
   "source": [
    "The second latent concept (Figure 6) is about inflation. The concept seems to cover the sources of inflation (as indicated by the presence of the terms *food* and *pressures*) as well as the Monetary Board's policy response to inflation via adjustments of the policy rate.\n",
    "\n",
    "<center>Figure 6. Inflation latent concept</center>\n",
    "\n",
    "<center><img src=\"sv2_final.png\" alt=\"sv2_final.png\" style=\"width:800px; height:600px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df454039-a08f-4f87-807c-efe0cc62df33",
   "metadata": {},
   "source": [
    "The third latent concept (Figure 7) is about the key performance indicators of the banking sector. The most important terms *percent* and *ratio* strongly support the notion that this cluster of words are about numerical indicators. The terms *npl* and *npls* (non-performing loan/s), *tlp* (total loan provision), *loans*, and *npa* (non-performing asset) are all banking sector performance metrics.\n",
    "\n",
    "\n",
    "<center>Figure 7. Banking sector key performance indicators latent concept</center>\n",
    "\n",
    "<center><img src=\"sv3_final.png\" alt=\"sv3_final.png\" style=\"width:800px; height:600px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31032e77-0b52-4a38-9922-b59b831b859f",
   "metadata": {},
   "source": [
    "We find our first overlap in the fourth and ninth latent concepts (Figure 8). They are about the BSP's annual awards and appreciation ceremony for outstanding regional stakeholders, with the ninth latent concept seemingly having a stronger association with the identified theme than the fourth. The BSP has been recognizing significant stakeholders and partner institutions from each region in the Philippines since 2007 and publishes a few press releases about these ceremonies annually.\n",
    "\n",
    "<center>Figure 8. Recognition of regional stakeholders latent concept</center>\n",
    "\n",
    "<center><img src=\"sv4_final.png\" alt=\"sv4_final.png\" style=\"width:800px; height:600px\"/> <img src=\"sv9_final.png\" alt=\"sv9_final.png\" style=\"width:800px; height:600px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032abb01-10d0-4154-a5a2-a0ea53c190e1",
   "metadata": {},
   "source": [
    "The fifth and sixth latent concepts (Figure 9) are also seemingly overlapping topics that refer to the BSP's rediscount facility, a standing credit facility used to help banks with their liqudity needs. The rediscount facility is one of the BSP's monetary tools to influence the level of credit in the financial system. There are two categories of rediscount facilities: the \"Peso Rediscount Facility\" and the \"Exporters Dollar and Yen Rediscount Facility\".\n",
    "\n",
    "<center>Figure 9. Rediscount facility latent concept</center>\n",
    "\n",
    "<center><img src=\"sv5_final.png\" alt=\"sv5_final.png\" style=\"width:800px; height:600px\"/> <img src=\"sv6_final.png\" alt=\"sv6_final.png\" style=\"width:800px; height:600px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea31ab-df8b-4c6f-b1b4-c72f45cce5af",
   "metadata": {},
   "source": [
    "The seventh latent concept (Figure 10) is very clearly about the remittances of overseas Filipino workers, deemed to be a key driver of growth and consumption in the Philippine economy. The BSP regularly reports the level of remittance inflows in a disaggregated manner (cash and personal remittances).\n",
    "\n",
    "<center>Figure 10. Remittances latent concept</center>\n",
    "\n",
    "<center><img src=\"sv7_final.png\" alt=\"sv7_final.png\" style=\"width:800px; height:600px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad7ff6b-a96d-4f39-8c9b-3e5cecb55fd7",
   "metadata": {},
   "source": [
    "The eight latent concept (Figure 11) is about the growth of domestic liquidity. Domestic liquidity is determined in part by the level of bank lending (or loans and credit) in the financial system and the level of net foreign assets (nfa) by the BSP and domestic banks.\n",
    "\n",
    "<center>Figure 11. Domestic liquidity latent concept</center>\n",
    "\n",
    "<center><img src=\"sv8_final.png\" alt=\"sv8_final.png\" style=\"width:800px; height:600px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91304344-8429-4350-bedd-38bf05b3d8b2",
   "metadata": {},
   "source": [
    "The tenth latent concept (Figure 12) is about market expectations. Market expectations are significant inputs to BSP policymaking. Every quarter, the BSP conducts surveys among households and firms to capture their expectations of the economy moving forward.\n",
    "\n",
    "<center>Figure 12. Market expectations latent concept</center>\n",
    "\n",
    "<center><img src=\"sv10_final.png\" alt=\"sv10_final.png\" style=\"width:800px; height:600px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5c5513-6ca0-480d-b28f-eff2d39ae545",
   "metadata": {},
   "source": [
    "### 4.3&emsp;Identifying standout concepts per year: A demonstration from 2013 to 2023 press releases\n",
    "\n",
    "We extract further practical value from this exercise by identifying the latent concepts that stand out per year. Doing so allows us to determine what economic and policy priorities had formed during specific years, narrating a timeline of central banking developments. As a demonstration, we perform this exercise on year-specific subsets of the corpus of press releases from 2013 to 2023. The results of this exercise are as follows.\n",
    "\n",
    "In 2013 and 2015, latent topics referring to the implementation of regulatory reliefs on banks appeared. This indicates that the BSP implemented regulatory relief measures for banks for some reason. Examining the press releases from these periods reveals that these measures were implemented to provide relief for banks because of the adverse effects of typhoons on the Philippine banking system.\n",
    "\n",
    "In 2017, topics referring to the ASEAN Banking Integration Framework and the phaseout of old generation banknotes stood out. It was during this time that the BSP's negotiations with ASEAN central banks regarding regional banking integration were being concluded. The full transition from the old to the new generation of banknotes was also taking place in 2017.\n",
    "\n",
    "In 2018, the Financial Stability Coordination Council (FSCC) stood out as a latent concept, signifying a strengthening of the BSP's commitment to preserving financial stability in the Philippine financial system.\n",
    "\n",
    "In 2019, themes on the release of new coins and reductions in reserve requirements for banks surfaced. The BSP released its new 20-Piso coins and enhanced 5-Piso coins into circulation in 2019. During this time, the Monetary Board also slashed the reserve requirement ratio for banks a few times, signifying the central bank's move toward a more expansionary monetary policy.\n",
    "\n",
    "In 2020, latent concepts referring to inflation outlooks and financial stability became prominent. It is worth noting that the BSP started releasing month-ahead inflation outlooks only in the latter half of 2018 and likely doubled down on the effort in 2020. Moreover, the BSP became more vocal about financial stability as the FSCC was institutionalized by an Executive Order by then-President Rodrigo R. Duterte during the year.\n",
    "\n",
    "In 2021, digitalization and, more specifically, digitalization were the standout themes of the year. The push towards digitalization was the BSP's response to the ongoing coronavirus-2019 pandemic.\n",
    "\n",
    "In 2022, topics referring to the release of the new polymer 1,000-Piso banknote and, once again, digital payments were standout topics. It was during this year that the BSP issued the Philippines' first polymer banknote into circulation. The topic of digital payments was also a continuing concern for the BSP, indicating the central bank's clear commitment toward making digital payments more prevalent in the country.\n",
    "\n",
    "Finally, in 2023, the BSP's Paleng-QR initiative, convictions against violators of the BSP's Manual of Regulations for Non-Bank Financial Institutions (MORNBFI), and Islamic banking were the new standout topics for the year. The Paleng-QR PH initiative is in line with the BSP's thrust to promote digital payments in the country, specifically in wet markets. The BSP also ramped up its efforts to pursue violators of the MORNBFI during the year. Islamic banking also became a top concern of the BSP in 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa20bf4-8f46-44f9-8ca5-37d18eb667c9",
   "metadata": {},
   "source": [
    "### 4.4&emsp;Identifying standout tokens per concept per year: A case of a shift in the inflation narrative\n",
    "\n",
    "We show some further value of our model by identifying tokens that stand out in certain latent concepts per year. We take the case of the latent concept of inflation to demonstrate.\n",
    "\n",
    "In the cluster of tokens referring to inflation, the term *supply* appeared for the first time among the top positive-association tokens when using the subset data for 2021 (Figure 13) and once again using the data for 2022 (Figure 14). This represents a material change in the inflation narrative of the BSP during these years. It should be noted that the Philippines' most recent inflationary episode (of which the height was felt in 2022) was driven mostly by supply-side factors beyond the BSP's realm of policy influence. Based on the appearance of the *supply* token in the latent theme referring to inflation, it seems that the BSP was keen on informing the public about this information.\n",
    "\n",
    "<center>Figure 13. Inflation latent concept (positive association tokens), 2021</center>\n",
    "\n",
    "<center><img src=\"svinf2021.png\" alt=\"svinf2021.png\" style=\"width:800px; height:600px\"/></center><center>\n",
    "    \n",
    "<center>Figure 14. Inflation latent concept (positive association tokens), 2022</center>\n",
    "\n",
    "<center><img src=\"svinf2022.png\" alt=\"svinf2022.png\" style=\"width:800px; height:600px\"/></center><center>\n",
    "\n",
    "One may apply the same line of thinking to other latent concepts non-varying over time to see how the narrative with respect to those themes change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a9f641-6ebe-45a6-b96f-51836fc12e68",
   "metadata": {},
   "source": [
    "## 4&emsp;Conclusion and Recommendation\n",
    "***\n",
    "\n",
    "In this report, we showed how LSA can be used to tell the a story of central banking history using the BSP's press releases as a case study. We determined the overarching themes of concern in BSP communications as well as standout concepts per year to demonstrate the potential of LSA to uncover changing narratives and policy priorities of the BSP. We also showed how focusing on standout tokens within recurring themes per year represents shifts in the narrative for those themes using inflation as an example theme. ALl in all, we showed how the mathematical underpinnings of SVD and LSA can be used to interpret historical text data to tell a story of dynamism in BSP policymaking.\n",
    "\n",
    "This work can be applied practically by creating a search-by-similarity information retrieval system for BSP press releases, effectively creating a press release recommender system for those viewing the press release materials via the BSP website. As of writing, the BSP does not have a recommender system for its press releases. It would be helpful to link viewers reading any single press release to similar articles in the BSP's repository. However, even without this, policymakers, researchers, and enthusiasts will find value in knowing about the timeline of BSP's policy communication priorities as they will be provided a good starting point from which to begin a search.\n",
    "\n",
    "Our work can be improved by applying a more quantitative approach in determining how common the singular values are per year. In this exercise, we relied on domain knowledge to decide if the seemingly common singular values we derived each year did, indeed, refer to the same latent concepts. For future work, we could derive similarity measures such as cosine similarity to judge if these latent concepts per year truly are similar enough to conclude that they refer to the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34b9894-6ff6-4c95-a4db-d63c3875e598",
   "metadata": {},
   "source": [
    "## References\n",
    "***\n",
    "\n",
    "Bangko Sentral ng Pilipinas. (n.d.). Media and Research. Bangko Sentral ng Pilipinas. https://www.bsp.gov.ph/SitePages/MediaAndResearch/MediaList.aspx?TabId=1/\n",
    "\n",
    "Ioana. (2020). Latent Semantic Analysis: intuition, math, implementation. Medium.com. https://towardsdatascience.com/latent-semantic-analysis-intuition-math-implementation-a194aff870f8/\n",
    "\n",
    "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., &amp; Duchesnay, É. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12(85), 2825–2830. https://jmlr.csail.mit.edu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8f8d91-e630-404f-907c-5797e307edc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
